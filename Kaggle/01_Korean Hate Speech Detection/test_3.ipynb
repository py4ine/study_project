{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "# from pykospacing import Spacing\n",
    "# import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'./data/korean-hate-speech-detection/train.hate.csv')\n",
    "\n",
    "train_text = open(r'./data/korean-hate-speech-detection/train.news_title.txt', 'r')\n",
    "train_texts = train_text.readlines()\n",
    "\n",
    "train_text_list = []\n",
    "for line in train_texts:\n",
    "    train_text_list.append(line)\n",
    "train_text.close()\n",
    "\n",
    "train_df['text'] = train_text_list\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengh = []\n",
    "for i in train_df['comments']:\n",
    "    lengh.append(len(i))\n",
    "max(lengh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = Spacing()\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = spacing(text)\n",
    "    text = text.lower()  # 소문자 변경\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# 기본 불용어 불러오기\n",
    "korean_stopwords_path = \"data/stopwords-ko.txt\"\n",
    "with open(korean_stopwords_path, encoding='utf8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip() for x in stopwords]\n",
    "\n",
    "# 불용어 처리 함수 수정\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()  # 문장을 단어로 분리\n",
    "    filtered_text = [word for word in words if word not in stopwords]  # 단어가 불용어 목록에 없는 경우만 추가\n",
    "    return ' '.join(filtered_text)  # 필터링된 단어들 다시 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(train_df))):\n",
    "    comment_text = train_df.loc[i, 'comments']\n",
    "    newstile_text = train_df.loc[i, 'text']\n",
    "    processed_comment = preprocessing(comment_text)\n",
    "    processed_newstile = preprocessing(newstile_text)\n",
    "    cleaned_comment = remove_stopwords(processed_comment, stopwords)\n",
    "    cleaned_newstile = remove_stopwords(processed_newstile, stopwords)\n",
    "    train_df.loc[i, 'processed_comments'] = cleaned_comment\n",
    "    train_df.loc[i, 'processed_newstitle'] = cleaned_newstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['label'] == 'none', 'labels'] = 0\n",
    "train_df.loc[train_df['label'] == 'offensive', 'labels'] = 1\n",
    "train_df.loc[train_df['label'] == 'hate', 'labels'] = 2\n",
    "\n",
    "train_df['labels'] = train_df['labels'].astype(int)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/train_df_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(r'./data/korean-hate-speech-detection/dev.hate.csv')\n",
    "\n",
    "dev_text = open(r'./data/korean-hate-speech-detection/dev.news_title.txt', 'r')\n",
    "dev_texts = dev_text.readlines()\n",
    "\n",
    "dev_text_list = []\n",
    "for line in dev_texts:\n",
    "    dev_text_list.append(line)\n",
    "dev_text.close\n",
    "\n",
    "dev_df['text'] = dev_text_list\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(dev_df))):\n",
    "    comment_text = dev_df.loc[i, 'comments']\n",
    "    newstile_text = dev_df.loc[i, 'text']\n",
    "    processed_comment = preprocessing(comment_text)\n",
    "    processed_newstile = preprocessing(newstile_text)\n",
    "    cleaned_comment = remove_stopwords(processed_comment, stopwords)\n",
    "    cleaned_newstile = remove_stopwords(processed_newstile, stopwords)\n",
    "    dev_df.loc[i, 'processed_comments'] = cleaned_comment\n",
    "    dev_df.loc[i, 'processed_newstitle'] = cleaned_newstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.loc[dev_df['label'] == 'none', 'labels'] = 0\n",
    "dev_df.loc[dev_df['label'] == 'offensive', 'labels'] = 1\n",
    "dev_df.loc[dev_df['label'] == 'hate', 'labels'] = 2\n",
    "dev_df['labels'] = dev_df['labels'].astype(int)\n",
    "\n",
    "dev_df.to_csv('./data/dev_df_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('./data/dev_df_processed.csv')\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(r'./data/korean-hate-speech-detection/test.hate.no_label.csv')\n",
    "\n",
    "test_text = open(r'./data/korean-hate-speech-detection/test.news_title.txt')\n",
    "test_texts = test_text.readlines()\n",
    "test_text_list = []\n",
    "for line in test_texts:\n",
    "    test_text_list.append(line)\n",
    "\n",
    "test_df['text'] = test_text_list\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    comment_text = test_df.loc[i, 'comments']\n",
    "    newstile_text = test_df.loc[i, 'text']\n",
    "    processed_comment = preprocessing(comment_text)\n",
    "    processed_newstile = preprocessing(newstile_text)\n",
    "    cleaned_comment = remove_stopwords(processed_comment, stopwords)\n",
    "    cleaned_newstile = remove_stopwords(processed_newstile, stopwords)\n",
    "    test_df.loc[i, 'processed_comments'] = cleaned_comment\n",
    "    test_df.loc[i, 'processed_newstitle'] = cleaned_newstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./data/test_df_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'./data/train_df_processed.csv')\n",
    "dev_df = pd.read_csv(r'./data/dev_df_processed.csv')\n",
    "test_df = pd.read_csv(r'./data/test_df_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'beomi/KcELECTRA-base-v2022'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 16:26:14.588878: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-11-01 16:26:14.588892: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-11-01 16:26:14.588899: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-11-01 16:26:14.589080: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-01 16:26:14.589091: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    tensor = tf.constant([1.0, 2.0, 3.0])\n",
    "print(\"device:\", tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 토크나이징\n",
    "\n",
    "tokenized_train_comment = tokenizer(\n",
    "    list(train_df['processed_comments']),\n",
    "    return_tensors='tf',\n",
    "    # max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,          # max_length 초과 토큰 truncate\n",
    "    add_special_tokens=True,  # special token 추가\n",
    ")\n",
    "\n",
    "tokenized_train_title = tokenizer(\n",
    "    list(train_df['processed_newstitle']),\n",
    "    return_tensors='tf',\n",
    "    # max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,          # max_length 초과 토큰 truncate\n",
    "    add_special_tokens=True,  # special token 추가\n",
    ")\n",
    "\n",
    "tokenized_dev_comment = tokenizer(\n",
    "    list(dev_df['processed_comments']),\n",
    "    return_tensors='tf',\n",
    "    # max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,          # max_length 초과 토큰 truncate\n",
    "    add_special_tokens=True,  # special token 추가\n",
    ")\n",
    "\n",
    "tokenized_dev_title = tokenizer(\n",
    "    list(dev_df['processed_newstitle']),\n",
    "    return_tensors='tf',\n",
    "    # max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,          # max_length 초과 토큰 truncate\n",
    "    add_special_tokens=True,  # special token 추가\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(7893, 512), dtype=int32, numpy=\n",
       "array([[    2,  9008, 15136, ...,     0,     0,     0],\n",
       "       [    2,  7991,  8065, ...,     0,     0,     0],\n",
       "       [    2, 11026, 11826, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    2,  9196, 33024, ...,     0,     0,     0],\n",
       "       [    2,  9196, 22597, ...,     0,     0,     0],\n",
       "       [    2, 24810,   966, ...,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(7893, 512), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(7893, 512), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(471, 512), dtype=int32, numpy=\n",
       "array([[    2, 53063,  9237, ...,     0,     0,     0],\n",
       "       [    2, 51783,  4194, ...,     0,     0,     0],\n",
       "       [    2,  8926,  9238, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    2,  8205, 28149, ...,     0,     0,     0],\n",
       "       [    2, 16679,  4826, ...,     0,     0,     0],\n",
       "       [    2, 10879, 13827, ...,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(471, 512), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(471, 512), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dev_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLS 토큰 벡터 추출\n",
    "train_comment_embedding = tf.reshape(tokenized_train_comment['input_ids'], (7893,-1))\n",
    "train_title_embedding = tf.reshape(tokenized_train_title['input_ids'], (-1,))\n",
    "dev_comment_embedding = tf.reshape(tokenized_dev_comment['input_ids'], (471,-1))\n",
    "dev_title_embedding = tf.reshape(tokenized_dev_title['input_ids'], (-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['labels'] = train_df['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 배열로 변환\n",
    "# X_train = tf.reshape(train_comment_embedding, (-1, 1))\n",
    "# X_test = tf.reshape(dev_comment_embedding, (-1, 1))     # 테스트 데이터도 2D로 변환\n",
    "y_train = train_df['labels']\n",
    "y_test = dev_df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7893, 512), dtype=int32, numpy=\n",
       "array([[    2,  9008, 15136, ...,     0,     0,     0],\n",
       "       [    2,  7991,  8065, ...,     0,     0,     0],\n",
       "       [    2, 11026, 11826, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    2,  9196, 33024, ...,     0,     0,     0],\n",
       "       [    2,  9196, 22597, ...,     0,     0,     0],\n",
       "       [    2, 24810,   966, ...,     0,     0,     0]], dtype=int32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(471, 512), dtype=int32, numpy=\n",
       "array([[    2, 53063,  9237, ...,     0,     0,     0],\n",
       "       [    2, 51783,  4194, ...,     0,     0,     0],\n",
       "       [    2,  8926,  9238, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    2,  8205, 28149, ...,     0,     0,     0],\n",
       "       [    2, 16679,  4826, ...,     0,     0,     0],\n",
       "       [    2, 10879, 13827, ...,     0,     0,     0]], dtype=int32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_comment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       0\n",
       "2       2\n",
       "3       0\n",
       "4       2\n",
       "       ..\n",
       "7891    0\n",
       "7892    0\n",
       "7893    0\n",
       "7894    0\n",
       "7895    0\n",
       "Name: labels, Length: 7893, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34394904458598724\n",
      "F1 score: 0.2500706524399621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀 모델 학습\n",
    "logistic_model = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "logistic_model.fit(train_comment_embedding, train_df['labels'])\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = logistic_model.predict(dev_comment_embedding)\n",
    "accuracy = accuracy_score(dev_df['labels'], y_pred)\n",
    "f1 = f1_score(dev_df['labels'], y_pred, average='weighted')  # 다중 클래스 분류의 경우 'weighted' 또는 'macro' 사용\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32908704883227174\n",
      "F1 Score: 0.22604433570296728\n"
     ]
    }
   ],
   "source": [
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_comment_embedding)\n",
    "X_test_scaled = scaler.transform(dev_comment_embedding)\n",
    "\n",
    "# 로지스틱 회귀 모델 초기화 및 학습\n",
    "logistic_model = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='saga')\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "y_pred = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# 성능 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(train_comment_embedding, y_train)\n",
    "y_pred = rf_model.predict(dev_comment_embedding)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # 다중 클래스 분류의 경우 'weighted' 또는 'macro' 사용\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectM4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
