{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "# from pykospacing import Spacing\n",
    "# import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'./data/korean-hate-speech-detection/train.hate.csv')\n",
    "\n",
    "train_text = open(r'./data/korean-hate-speech-detection/train.news_title.txt', 'r')\n",
    "train_texts = train_text.readlines()\n",
    "\n",
    "train_text_list = []\n",
    "for line in train_texts:\n",
    "    train_text_list.append(line)\n",
    "train_text.close()\n",
    "\n",
    "train_df['text'] = train_text_list\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengh = []\n",
    "for i in train_df['comments']:\n",
    "    lengh.append(len(i))\n",
    "max(lengh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = Spacing()\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = spacing(text)\n",
    "    text = text.lower()  # 소문자 변경\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# 기본 불용어 불러오기\n",
    "korean_stopwords_path = \"data/stopwords-ko.txt\"\n",
    "with open(korean_stopwords_path, encoding='utf8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip() for x in stopwords]\n",
    "\n",
    "# 불용어 처리 함수 수정\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()  # 문장을 단어로 분리\n",
    "    filtered_text = [word for word in words if word not in stopwords]  # 단어가 불용어 목록에 없는 경우만 추가\n",
    "    return ' '.join(filtered_text)  # 필터링된 단어들 다시 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(train_df))):\n",
    "    comment_text = train_df.loc[i, 'comments']\n",
    "    newstile_text = train_df.loc[i, 'text']\n",
    "    processed_comment = preprocessing(comment_text)\n",
    "    processed_newstile = preprocessing(newstile_text)\n",
    "    cleaned_comment = remove_stopwords(processed_comment, stopwords)\n",
    "    cleaned_newstile = remove_stopwords(processed_newstile, stopwords)\n",
    "    train_df.loc[i, 'processed_comments'] = cleaned_comment\n",
    "    train_df.loc[i, 'processed_newstitle'] = cleaned_newstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['label'] == 'none', 'labels'] = 0\n",
    "train_df.loc[train_df['label'] == 'offensive', 'labels'] = 1\n",
    "train_df.loc[train_df['label'] == 'hate', 'labels'] = 2\n",
    "\n",
    "train_df['labels'] = train_df['labels'].astype(int)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/train_df_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(r'./data/korean-hate-speech-detection/dev.hate.csv')\n",
    "\n",
    "dev_text = open(r'./data/korean-hate-speech-detection/dev.news_title.txt', 'r')\n",
    "dev_texts = dev_text.readlines()\n",
    "\n",
    "dev_text_list = []\n",
    "for line in dev_texts:\n",
    "    dev_text_list.append(line)\n",
    "dev_text.close\n",
    "\n",
    "dev_df['text'] = dev_text_list\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(dev_df))):\n",
    "    comment_text = dev_df.loc[i, 'comments']\n",
    "    newstile_text = dev_df.loc[i, 'text']\n",
    "    processed_comment = preprocessing(comment_text)\n",
    "    processed_newstile = preprocessing(newstile_text)\n",
    "    cleaned_comment = remove_stopwords(processed_comment, stopwords)\n",
    "    cleaned_newstile = remove_stopwords(processed_newstile, stopwords)\n",
    "    dev_df.loc[i, 'processed_comments'] = cleaned_comment\n",
    "    dev_df.loc[i, 'processed_newstitle'] = cleaned_newstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.loc[dev_df['label'] == 'none', 'labels'] = 0\n",
    "dev_df.loc[dev_df['label'] == 'offensive', 'labels'] = 1\n",
    "dev_df.loc[dev_df['label'] == 'hate', 'labels'] = 2\n",
    "dev_df['labels'] = dev_df['labels'].astype(int)\n",
    "\n",
    "dev_df.to_csv('./data/dev_df_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('./data/dev_df_processed.csv')\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(r'./data/korean-hate-speech-detection/test.hate.no_label.csv')\n",
    "\n",
    "test_text = open(r'./data/korean-hate-speech-detection/test.news_title.txt')\n",
    "test_texts = test_text.readlines()\n",
    "test_text_list = []\n",
    "for line in test_texts:\n",
    "    test_text_list.append(line)\n",
    "\n",
    "test_df['text'] = test_text_list\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    comment_text = test_df.loc[i, 'comments']\n",
    "    newstile_text = test_df.loc[i, 'text']\n",
    "    processed_comment = preprocessing(comment_text)\n",
    "    processed_newstile = preprocessing(newstile_text)\n",
    "    cleaned_comment = remove_stopwords(processed_comment, stopwords)\n",
    "    cleaned_newstile = remove_stopwords(processed_newstile, stopwords)\n",
    "    test_df.loc[i, 'processed_comments'] = cleaned_comment\n",
    "    test_df.loc[i, 'processed_newstitle'] = cleaned_newstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./data/test_df_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'./data/train_df_processed.csv')\n",
    "dev_df = pd.read_csv(r'./data/dev_df_processed.csv')\n",
    "test_df = pd.read_csv(r'./data/test_df_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/tfGPU/lib/python3.9/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'beomi/KcELECTRA-base-v2022'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 13:47:10.513057: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-10-27 13:47:10.513437: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-10-27 13:47:10.513442: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-10-27 13:47:10.513741: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-27 13:47:10.514102: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    tensor = tf.constant([1.0, 2.0, 3.0])\n",
    "print(\"device:\", tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_comments</th>\n",
       "      <th>processed_newstitle</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속...</td>\n",
       "      <td>hate</td>\n",
       "      <td>\"밤새 조문 행렬…故 전미선, 동료들이 그리워하는 따뜻한 배우 [종합]\"\\n</td>\n",
       "      <td>현재 호텔 주인 심정 아18 난 마른 하늘에 날 벼락 맞고 호텔 망하게 생겼는데 누...</td>\n",
       "      <td>밤새 조문 행렬 故 전미선 동료들이 그리워하는 따뜻한 배우 종합</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...</td>\n",
       "      <td>none</td>\n",
       "      <td>\"'연중' 故 전미선, 생전 마지막 미공개 인터뷰…환하게 웃는 모습 '먹먹'[종합]\"\\n</td>\n",
       "      <td>한국적인 미인의 대표적인 분 너무나 곱고 아름다운 모습그 모습 뒤의 슬픔을 미 처 ...</td>\n",
       "      <td>연중 故 전미선 생전 마지막 미공개 인터뷰 환하게 웃는 모습 먹먹종합</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...못된 넘들...남의 고통을 즐겼던 넘들..이젠 마땅한 처벌을 받아야지..,그래...</td>\n",
       "      <td>hate</td>\n",
       "      <td>\"[단독] 잔나비, 라디오 출연 취소→'한밤' 방송 연기..비판 여론 ing(종합)\"\\n</td>\n",
       "      <td>못된 넘들남의 고통을 즐겼던 넘들이젠 마땅한 처벌을 받아야지 그래야 공정한 사회지심...</td>\n",
       "      <td>단독 잔나비 라디오 출연 취소한밤 방송 연기비판 여론 ing종합</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데</td>\n",
       "      <td>none</td>\n",
       "      <td>\"'아스달 연대기' 장동건-김옥빈, 들끓는 '욕망커플'→눈물범벅 '칼끝 대립'\"\\n</td>\n",
       "      <td>2화 어설 펐는데 4화 지나서부터는 갈수록 너무 재밌던 데</td>\n",
       "      <td>아스달 연대기 장동건김옥빈 들끓는 욕망 커플 눈물범벅 칼끝 대립</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각...</td>\n",
       "      <td>hate</td>\n",
       "      <td>[DA:이슈] ‘구하라 비보’ 최종범 항소심에 영향?…법조계 “‘공소권 없음’ 아냐”\\n</td>\n",
       "      <td>사람 얼굴 손톱으로 긁은 것은 인격 살해이고 동영상이 몰카냐 메걸리안 생각이 없노</td>\n",
       "      <td>da 이슈 구하라 비보 최종 범 항소심에 영향 법조계 공소권 없음 아냐</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>힘내세요~ 응원합니다!!</td>\n",
       "      <td>none</td>\n",
       "      <td>\"허지웅, 허투루 넘길 말 없었다…솔직하게 드러냈던 속사정\"\\n</td>\n",
       "      <td>힘내세요 응원합니다</td>\n",
       "      <td>허지웅 허투루 넘길 말 없었다 솔직하게 드러냈던 속 사정</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7892</th>\n",
       "      <td>힘내세요~~삼가 고인의 명복을 빕니다..</td>\n",
       "      <td>none</td>\n",
       "      <td>\"이혜경, ‘오! 캐롤’ 공연 중 남편 오정욱 부고…오열 속 발인 [종합]\"\\n</td>\n",
       "      <td>힘내세요삼가 고인의 명복을 빕니다</td>\n",
       "      <td>이혜경 캐롤 공연 중 남편 오정욱 부고오열 속 발인 종합</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7893</th>\n",
       "      <td>힘내세용 ^^ 항상 응원합니닷 ^^ !</td>\n",
       "      <td>none</td>\n",
       "      <td>\"'설경구♥' 송윤아, 아들과 즐거운 하루 \"\"전 엄마니까요\"\"\"\\n</td>\n",
       "      <td>힘내세용 항상 응원합니닷</td>\n",
       "      <td>설경구 송윤아 아들과 즐거운 하루 전 엄마니까요</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>힘내소...연기로 답해요.나도 53살 인데 이런일 저런일 다 있더라구요.인격을 믿습...</td>\n",
       "      <td>none</td>\n",
       "      <td>\"[SC현장]\"\"연예인 인생 협박 유감\"\"…미소잃은 최민수, '보복운전 혐의' 2차...</td>\n",
       "      <td>힘내소연기로 답해요 나도 53살 인데 저런 다 있더라구요인 격을 믿습니다홨팅</td>\n",
       "      <td>sc현장연예인 인생 협박 유감 미소 잃은 최민수 보복운전 혐의 2차 공판종합</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>힘들면 관뒀어야지 그게 현명한거다</td>\n",
       "      <td>none</td>\n",
       "      <td>\"[단독]스태프 사망사고 '서른이지만', 결국 오늘 촬영 취소\"\\n</td>\n",
       "      <td>힘들면 관뒀어야지 그게 현명한 거다</td>\n",
       "      <td>단독 스태프 사망사고 서른이지만 오늘 촬영 취소</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7893 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments label  \\\n",
       "0     (현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속...  hate   \n",
       "1     ....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...  none   \n",
       "2     ...못된 넘들...남의 고통을 즐겼던 넘들..이젠 마땅한 처벌을 받아야지..,그래...  hate   \n",
       "3                    1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데  none   \n",
       "4     1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각...  hate   \n",
       "...                                                 ...   ...   \n",
       "7891                                      힘내세요~ 응원합니다!!  none   \n",
       "7892                             힘내세요~~삼가 고인의 명복을 빕니다..  none   \n",
       "7893                              힘내세용 ^^ 항상 응원합니닷 ^^ !  none   \n",
       "7894  힘내소...연기로 답해요.나도 53살 인데 이런일 저런일 다 있더라구요.인격을 믿습...  none   \n",
       "7895                                 힘들면 관뒀어야지 그게 현명한거다  none   \n",
       "\n",
       "                                                   text  \\\n",
       "0            \"밤새 조문 행렬…故 전미선, 동료들이 그리워하는 따뜻한 배우 [종합]\"\\n   \n",
       "1     \"'연중' 故 전미선, 생전 마지막 미공개 인터뷰…환하게 웃는 모습 '먹먹'[종합]\"\\n   \n",
       "2     \"[단독] 잔나비, 라디오 출연 취소→'한밤' 방송 연기..비판 여론 ing(종합)\"\\n   \n",
       "3        \"'아스달 연대기' 장동건-김옥빈, 들끓는 '욕망커플'→눈물범벅 '칼끝 대립'\"\\n   \n",
       "4     [DA:이슈] ‘구하라 비보’ 최종범 항소심에 영향?…법조계 “‘공소권 없음’ 아냐”\\n   \n",
       "...                                                 ...   \n",
       "7891                \"허지웅, 허투루 넘길 말 없었다…솔직하게 드러냈던 속사정\"\\n   \n",
       "7892       \"이혜경, ‘오! 캐롤’ 공연 중 남편 오정욱 부고…오열 속 발인 [종합]\"\\n   \n",
       "7893             \"'설경구♥' 송윤아, 아들과 즐거운 하루 \"\"전 엄마니까요\"\"\"\\n   \n",
       "7894  \"[SC현장]\"\"연예인 인생 협박 유감\"\"…미소잃은 최민수, '보복운전 혐의' 2차...   \n",
       "7895              \"[단독]스태프 사망사고 '서른이지만', 결국 오늘 촬영 취소\"\\n   \n",
       "\n",
       "                                     processed_comments  \\\n",
       "0     현재 호텔 주인 심정 아18 난 마른 하늘에 날 벼락 맞고 호텔 망하게 생겼는데 누...   \n",
       "1     한국적인 미인의 대표적인 분 너무나 곱고 아름다운 모습그 모습 뒤의 슬픔을 미 처 ...   \n",
       "2     못된 넘들남의 고통을 즐겼던 넘들이젠 마땅한 처벌을 받아야지 그래야 공정한 사회지심...   \n",
       "3                      2화 어설 펐는데 4화 지나서부터는 갈수록 너무 재밌던 데   \n",
       "4         사람 얼굴 손톱으로 긁은 것은 인격 살해이고 동영상이 몰카냐 메걸리안 생각이 없노   \n",
       "...                                                 ...   \n",
       "7891                                         힘내세요 응원합니다   \n",
       "7892                                 힘내세요삼가 고인의 명복을 빕니다   \n",
       "7893                                      힘내세용 항상 응원합니닷   \n",
       "7894         힘내소연기로 답해요 나도 53살 인데 저런 다 있더라구요인 격을 믿습니다홨팅   \n",
       "7895                                힘들면 관뒀어야지 그게 현명한 거다   \n",
       "\n",
       "                             processed_newstitle  labels  \n",
       "0            밤새 조문 행렬 故 전미선 동료들이 그리워하는 따뜻한 배우 종합     2.0  \n",
       "1         연중 故 전미선 생전 마지막 미공개 인터뷰 환하게 웃는 모습 먹먹종합     0.0  \n",
       "2            단독 잔나비 라디오 출연 취소한밤 방송 연기비판 여론 ing종합     2.0  \n",
       "3            아스달 연대기 장동건김옥빈 들끓는 욕망 커플 눈물범벅 칼끝 대립     0.0  \n",
       "4        da 이슈 구하라 비보 최종 범 항소심에 영향 법조계 공소권 없음 아냐     2.0  \n",
       "...                                          ...     ...  \n",
       "7891             허지웅 허투루 넘길 말 없었다 솔직하게 드러냈던 속 사정     0.0  \n",
       "7892             이혜경 캐롤 공연 중 남편 오정욱 부고오열 속 발인 종합     0.0  \n",
       "7893                  설경구 송윤아 아들과 즐거운 하루 전 엄마니까요     0.0  \n",
       "7894  sc현장연예인 인생 협박 유감 미소 잃은 최민수 보복운전 혐의 2차 공판종합     0.0  \n",
       "7895                  단독 스태프 사망사고 서른이지만 오늘 촬영 취소     0.0  \n",
       "\n",
       "[7893 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 토크나이징\n",
    "\n",
    "tokenized_train_comment = tokenizer(\n",
    "    list(train_df['processed_comments']),\n",
    "    return_tensors='tf',\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,          # max_length 초과 토큰 truncate\n",
    "    add_special_tokens=True,  # special token 추가\n",
    ")\n",
    "\n",
    "tokenized_train_title = tokenizer(\n",
    "    list(train_df['processed_newstitle']),\n",
    "    return_tensors='tf',\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,          # max_length 초과 토큰 truncate\n",
    "    add_special_tokens=True,  # special token 추가\n",
    ")\n",
    "\n",
    "tokenized_dev_comment = tokenizer(\n",
    "    list(dev_df['processed_comments']),\n",
    "    return_tensors='tf',\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,          # max_length 초과 토큰 truncate\n",
    "    add_special_tokens=True,  # special token 추가\n",
    ")\n",
    "\n",
    "tokenized_dev_title = tokenizer(\n",
    "    list(dev_df['processed_newstitle']),\n",
    "    return_tensors='tf',\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,          # max_length 초과 토큰 truncate\n",
    "    add_special_tokens=True,  # special token 추가\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLS 토큰 벡터 추출\n",
    "train_comment_embedding = tokenized_train_comment['input_ids'][:, 0]\n",
    "train_title_embedding = tokenized_train_title['input_ids'][:, 0]\n",
    "dev_comment_embedding = tokenized_dev_comment['input_ids'][:, 0]\n",
    "dev_title_embedding = tokenized_dev_title['input_ids'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['labels'] = train_df['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 배열로 변환\n",
    "X_train = tf.reshape(train_comment_embedding, (-1, 1))\n",
    "X_test = tf.reshape(dev_comment_embedding, (-1, 1))     # 테스트 데이터도 2D로 변환\n",
    "y_train = train_df['labels']\n",
    "y_test = dev_df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33970276008492567\n",
      "F1 score: 0.1722739829273791\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀 모델 학습\n",
    "logistic_model = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "logistic_model.fit(X_train, train_df['labels'])\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "accuracy = accuracy_score(dev_df['labels'], y_pred)\n",
    "f1 = f1_score(dev_df['labels'], y_pred, average='weighted')  # 다중 클래스 분류의 경우 'weighted' 또는 'macro' 사용\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33970276008492567\n",
      "F1 score: 0.1722739829273791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # 다중 클래스 분류의 경우 'weighted' 또는 'macro' 사용\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(471, 1), dtype=int32, numpy=\n",
       "array([[2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]], dtype=int32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      2\n",
       "3      2\n",
       "4      1\n",
       "      ..\n",
       "466    1\n",
       "467    2\n",
       "468    1\n",
       "469    2\n",
       "470    0\n",
       "Name: labels, Length: 471, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectM4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
