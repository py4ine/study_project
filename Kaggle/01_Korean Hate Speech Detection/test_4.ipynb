{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraForSequenceClassification, ElectraTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# from pykospacing import Spacing\n",
    "# import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'./data/korean-hate-speech-detection/train.hate.csv')\n",
    "\n",
    "train_text = open(r'./data/korean-hate-speech-detection/train.news_title.txt', 'r')\n",
    "train_texts = train_text.readlines()\n",
    "\n",
    "train_text_list = []\n",
    "for line in train_texts:\n",
    "    train_text_list.append(line)\n",
    "train_text.close()\n",
    "\n",
    "train_df['text'] = train_text_list\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengh = []\n",
    "for i in train_df['comments']:\n",
    "    lengh.append(len(i))\n",
    "max(lengh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = Spacing()\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = spacing(text)\n",
    "    text = text.lower()  # 소문자 변경\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# 기본 불용어 불러오기\n",
    "korean_stopwords_path = \"data/stopwords-ko.txt\"\n",
    "with open(korean_stopwords_path, encoding='utf8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip() for x in stopwords]\n",
    "\n",
    "# 불용어 처리 함수 수정\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()  # 문장을 단어로 분리\n",
    "    filtered_text = [word for word in words if word not in stopwords]  # 단어가 불용어 목록에 없는 경우만 추가\n",
    "    return ' '.join(filtered_text)  # 필터링된 단어들 다시 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(train_df))):\n",
    "    comment_text = train_df.loc[i, 'comments']\n",
    "    newstile_text = train_df.loc[i, 'text']\n",
    "    processed_comment = preprocessing(comment_text)\n",
    "    processed_newstile = preprocessing(newstile_text)\n",
    "    cleaned_comment = remove_stopwords(processed_comment, stopwords)\n",
    "    cleaned_newstile = remove_stopwords(processed_newstile, stopwords)\n",
    "    train_df.loc[i, 'processed_comments'] = cleaned_comment\n",
    "    train_df.loc[i, 'processed_newstitle'] = cleaned_newstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['label'] == 'none', 'labels'] = 0\n",
    "train_df.loc[train_df['label'] == 'offensive', 'labels'] = 1\n",
    "train_df.loc[train_df['label'] == 'hate', 'labels'] = 2\n",
    "\n",
    "train_df['labels'] = train_df['labels'].astype(int)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/train_df_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(r'./data/korean-hate-speech-detection/dev.hate.csv')\n",
    "\n",
    "dev_text = open(r'./data/korean-hate-speech-detection/dev.news_title.txt', 'r')\n",
    "dev_texts = dev_text.readlines()\n",
    "\n",
    "dev_text_list = []\n",
    "for line in dev_texts:\n",
    "    dev_text_list.append(line)\n",
    "dev_text.close\n",
    "\n",
    "dev_df['text'] = dev_text_list\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(dev_df))):\n",
    "    comment_text = dev_df.loc[i, 'comments']\n",
    "    newstile_text = dev_df.loc[i, 'text']\n",
    "    processed_comment = preprocessing(comment_text)\n",
    "    processed_newstile = preprocessing(newstile_text)\n",
    "    cleaned_comment = remove_stopwords(processed_comment, stopwords)\n",
    "    cleaned_newstile = remove_stopwords(processed_newstile, stopwords)\n",
    "    dev_df.loc[i, 'processed_comments'] = cleaned_comment\n",
    "    dev_df.loc[i, 'processed_newstitle'] = cleaned_newstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.loc[dev_df['label'] == 'none', 'labels'] = 0\n",
    "dev_df.loc[dev_df['label'] == 'offensive', 'labels'] = 1\n",
    "dev_df.loc[dev_df['label'] == 'hate', 'labels'] = 2\n",
    "dev_df['labels'] = dev_df['labels'].astype(int)\n",
    "\n",
    "dev_df.to_csv('./data/dev_df_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('./data/dev_df_processed.csv')\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(r'./data/korean-hate-speech-detection/test.hate.no_label.csv')\n",
    "\n",
    "test_text = open(r'./data/korean-hate-speech-detection/test.news_title.txt')\n",
    "test_texts = test_text.readlines()\n",
    "test_text_list = []\n",
    "for line in test_texts:\n",
    "    test_text_list.append(line)\n",
    "\n",
    "test_df['text'] = test_text_list\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    comment_text = test_df.loc[i, 'comments']\n",
    "    newstile_text = test_df.loc[i, 'text']\n",
    "    processed_comment = preprocessing(comment_text)\n",
    "    processed_newstile = preprocessing(newstile_text)\n",
    "    cleaned_comment = remove_stopwords(processed_comment, stopwords)\n",
    "    cleaned_newstile = remove_stopwords(processed_newstile, stopwords)\n",
    "    test_df.loc[i, 'processed_comments'] = cleaned_comment\n",
    "    test_df.loc[i, 'processed_newstitle'] = cleaned_newstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./data/test_df_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'./data/train_df_processed.csv')\n",
    "dev_df = pd.read_csv(r'./data/dev_df_processed.csv')\n",
    "test_df = pd.read_csv(r'./data/test_df_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의 시, labels가 시리즈 형태라 오류 발생하여, tolist() 추가\n",
    "# 해도 오류라서 먼저 변경 해봄.\n",
    "train_title_list = train_df['text'].tolist()\n",
    "train_comment_list = train_df['comments'].tolist()\n",
    "train_label_list = train_df['labels'].astype(int).tolist()\n",
    "dev_title_list = dev_df['text'].tolist()\n",
    "dev_comment_list = dev_df['comments'].tolist()\n",
    "dev_label_list = dev_df['labels'].astype(int).tolist()\n",
    "print(type(train_label_list))\n",
    "print(type(dev_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'beomi/KcELECTRA-base-v2022'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class TitleCommentDataset(Dataset):\n",
    "    def __init__(self, titles, comments, labels, tokenizer, max_length=256):\n",
    "        self.titles = titles\n",
    "        self.comments = comments\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            return self.get_batch(idx)\n",
    "        return self.get_single_item(idx)\n",
    "    \n",
    "    def get_single_item(self, idx):\n",
    "        title = self.titles[idx]\n",
    "        comment = self.comments[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 제목과 댓글을 특별 토큰으로 구분하여 결합\n",
    "        text = f'{title} [SEP] {comment}'\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def get_batch(self, indices):\n",
    "        batch = {\n",
    "            'input_ids': [],\n",
    "            'attention_mask': [],\n",
    "            'labels': [],\n",
    "        }\n",
    "        for idx in indices:\n",
    "            item = self.get_single_item(idx)\n",
    "            for key in batch:\n",
    "                batch[key].append(item[key])\n",
    "    \n",
    "        return {key: torch.stack(batch[key]) for key in batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 및 데이터로더 생성\n",
    "train_dataset = TitleCommentDataset(train_title_list, train_comment_list, train_label_list, tokenizer)\n",
    "val_dataset = TitleCommentDataset(dev_title_list, dev_comment_list, dev_label_list, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의\n",
    "def train(model, train_loader, val_loader, epochs=3):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 검증\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = correct / total\n",
    "        print(f'Epoch {epoch+1}/{epoch}, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "train(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 수행\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # F1 스코어 계산\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    # 혼동 행렬 계산 및 시각화\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    disp.plot(ax=ax)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# 모델 평가 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "def make_tensors_contiguous(model):\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.contiguous()\n",
    "\n",
    "# 모델 저장 전에 호출\n",
    "make_tensors_contiguous(model)\n",
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 함수 정의\n",
    "def predict(title, comment):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    text = f\"{title} [SEP] {comment}\"\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=256,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "    \n",
    "    return predicted.item()  # 0, 1, 또는 2를 반환\n",
    "    \n",
    "# 예측 결과를 저장할 새로운 컬럼 생성\n",
    "test_df['predicted_label'] = None\n",
    "\n",
    "# tqdm을 사용하여 진행 상황 표시\n",
    "for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    title = row['text']  # CSV 파일의 제목 컬럼 이름\n",
    "    comment = row['comments']  # CSV 파일의 댓글 컬럼 이름\n",
    "    \n",
    "    # 예측 수행\n",
    "    predicted_label = predict(title, comment)\n",
    "    \n",
    "    # 예측 결과를 DataFrame에 저장\n",
    "    test_df.at[index, 'predicted_label'] = predicted_label\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "test_df.to_csv('save_results.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었고 결과가 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectM4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
