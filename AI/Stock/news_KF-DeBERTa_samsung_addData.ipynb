{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a38d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 코랩 javascript 멈춤 방지 코드\n",
    "# var startClickConnect = function startClickConnect(){\n",
    "#     var clickConnect = function clickConnect(){\n",
    "#         console.log(\"Connnect Clicked - Start\");\n",
    "#         document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
    "#         console.log(\"Connnect Clicked - End\");\n",
    "#     };\n",
    "\n",
    "#     var intervalId = setInterval(clickConnect, 60000*30);\n",
    "\n",
    "#     var stopClickConnectHandler = function stopClickConnect() {\n",
    "#         console.log(\"Connnect Clicked Stopped - Start\");\n",
    "#         clearInterval(intervalId);\n",
    "#         console.log(\"Connnect Clicked Stopped - End\");\n",
    "#     };\n",
    "\n",
    "#     return stopClickConnectHandler;\n",
    "# };\n",
    "\n",
    "# var stopClickConnect = startClickConnect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1e2ae-07eb-4b55-8062-6a00e421a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from pykospacing import Spacing\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27a7d2-c027-494f-bb67-9a0e848c1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KF-DeBERTa 모델\n",
    "model_name = 'kakaobank/kf-deberta-base'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "Classification_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2진분류\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbbdee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_excel('./data/New_samsung_news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d23091",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[data_df['Outcome'] == '악재', 'labels'] = 0\n",
    "data_df.loc[data_df['Outcome'] == '호재', 'labels'] = 1\n",
    "data_df['labels'] = data_df['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a19402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기, 대소문자 보정 함수\n",
    "spacing = Spacing()\n",
    "def preprocessing(text):\n",
    "    text = spacing(text)\n",
    "    text = text.lower()  # 소문자 변경\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# 기본 불용어 불러오기\n",
    "korean_stopwords_path = \"data/stopwords-ko.txt\"\n",
    "with open(korean_stopwords_path, encoding='utf8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip() for x in stopwords]\n",
    "\n",
    "# # 불용어 처리 함수\n",
    "# def remove_stopwords(text, stopwords):\n",
    "#     words = text.split()\n",
    "#     filtered_text = []\n",
    "#     for word in words:\n",
    "#         if text not in stopwords:\n",
    "#             filtered_text.append(text)\n",
    "#     return ' '.join(filtered_text)\n",
    "\n",
    "# 불용어 처리 함수 수정\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()  # 문장을 단어로 분리\n",
    "    filtered_text = [word for word in words if word not in stopwords]  # 단어가 불용어 목록에 없는 경우만 추가\n",
    "    return ' '.join(filtered_text)  # 필터링된 단어들 다시 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 문장을 단위로 분리하고, 각 문장에 단어 순서 변경 적용\n",
    "def augment_paragraph(paragraph):\n",
    "    # 문단을 문장으로 분리\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    aug = naw.RandomWordAug(action=\"swap\", aug_p=0.3)  # aug_p로 변경할 비율 설정 (30%)\n",
    "\n",
    "    augmented_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # 마침표로 끝나는지 확인하고, 있으면 따로 저장\n",
    "        has_period = sentence.endswith('.')\n",
    "        \n",
    "        # 단어 순서 변경 적용\n",
    "        augmented_sentence = aug.augment(sentence)[0]\n",
    "        \n",
    "        # 중복 마침표 제거 및 마침표 유지\n",
    "        augmented_sentence = augmented_sentence.rstrip('.')\n",
    "        if has_period:\n",
    "            augmented_sentence += '.'\n",
    "        \n",
    "        augmented_sentences.append(augmented_sentence)\n",
    "\n",
    "    # 문단으로 다시 조합\n",
    "    return ' '.join(augmented_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c32def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강 수행\n",
    "for i in (range(len(data_df))):\n",
    "    feature_text = data_df.loc[i, 'summary_content']  # 해당 행의 텍스트 가져오기\n",
    "    augmented_text = augment_paragraph(feature_text)\n",
    "    \n",
    "    # 해당 행의 'processed' 컬럼에 저장\n",
    "    data_df.loc[i, 'change_text'] = augmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d61b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_excel('./data/data_samsung_change.xlsx')\n",
    "\n",
    "# 저장된 엑셀파일 수기로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(data_df))):\n",
    "    feature_text = data_df.loc[i, 'summary_content']\n",
    "    processed_text = preprocessing(feature_text)\n",
    "    cleaned_text = remove_stopwords(processed_text, stopwords)\n",
    "    data_df.loc[i, 'processed'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_excel('./data/data_samsung_processed.xlsx')\n",
    "# data_df = pd.read_excel('./data/data_samsung_processed.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6e943c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KF_DeBERT모델 텍스트 토큰화\n",
    "encodings = tokenizer(data_df['processed'].tolist(), truncation=True, padding=True)  # truncation=최대길이를 초과하는 것에 대한 처리\n",
    "data_df['encoding'] = encodings\n",
    "\n",
    "# 데이터셋 준비\n",
    "data_df['input_ids'] = encodings['input_ids']  # input_ids=문장을 토크나이저가 일정 단위로 쪼개서 vocab에 들어있는 숫자로 치환한 것\n",
    "data_df['attention_mask'] = encodings['attention_mask']  # attention_mask=attention대상인지 아닌지를 나타냄. 대상이면1, 아니면0, 패딩된건 0으로 학습대상x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_excel('./data/data_apple_encoding.xlsx')\n",
    "# data_df = pd.read_excel('./data/data_apple_encoding.xlsx')\n",
    "\n",
    "# # 문자열을 리스트로 변환\n",
    "# data_df['input_ids'] = data_df['input_ids'].str.strip('[]').str.split(', ').apply(lambda x: list(map(int, x)))\n",
    "# data_df['attention_mask'] = data_df['attention_mask'].str.strip('[]').str.split(', ').apply(lambda x: list(map(int, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 비중 조정\n",
    "total_sample = len(data_df)\n",
    "negative = data_df[data_df['labels']==0]  # 부정(악재) 클래스 5,849\n",
    "positive = data_df[data_df['labels']==1]  # 긍정(호재) 클래스 10,672\n",
    "\n",
    "# 데이터 비율 설정\n",
    "total_rate = 1\n",
    "negative_rate = 1.60\n",
    "positive_rate = 0.80\n",
    "negative_sample_size = round(len(negative) * total_rate * negative_rate)\n",
    "positive_sample_size = round(len(positive) * total_rate * positive_rate)\n",
    "print(negative_sample_size)\n",
    "print(positive_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07991b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 선택\n",
    "negative_data = random.sample(list(negative.index), negative_sample_size-len(negative))\n",
    "add_negative = list(pd.concat([negative, negative.loc[negative_data]]).index)\n",
    "# negative_data = list(negative.index)\n",
    "positive_data = list(positive.index)\n",
    "\n",
    "# 최종 데이터 인덱스\n",
    "sample_index = positive_data + add_negative\n",
    "# sample_index = positive_data + negative_data\n",
    "random.shuffle(sample_index)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "sample_df = data_df.loc[sample_index]\n",
    "\n",
    "# train_test_split 데이터set\n",
    "train_columns = sample_df[['summary_content', 'input_ids', 'attention_mask']]\n",
    "test_colums = sample_df[['labels']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_columns, test_colums, test_size=0.2)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(X_train.join(y_train))\n",
    "test_dataset = Dataset.from_pandas(X_test.join(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8eecaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'learning_rate': [5e-5],  # 2e-5, 3e-5, 5e-5\n",
    "    'per_device_train_batch_size': [8],\n",
    "    'num_train_epochs': [3]\n",
    "}\n",
    "\n",
    "# # 하이퍼파라미터 옵션 설정\n",
    "# param_grid = {\n",
    "#     'learning_rate': [3e-5, 2e-5, 5e-5],  # 학습률\n",
    "#     'per_device_train_batch_size': [4, 8, 16, 32],  # 베치크기\n",
    "#     'num_train_epochs': [2, 4, 6, 8]  # 에포크 수\n",
    "# }\n",
    "\n",
    "# Best Parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'num_train_epochs': 8}\n",
    "# Best Accuracy: 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fcd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 초기화\n",
    "best_accuracy = 0\n",
    "best_F1 = 0\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "for learning_rate in tqdm(param_grid['learning_rate']):\n",
    "    for batch_size in param_grid['per_device_train_batch_size']:\n",
    "        for epochs in param_grid['num_train_epochs']:\n",
    "            try:\n",
    "                if batch_size > len(train_dataset):\n",
    "                    print(f\"Skipping batch size {batch_size} as it is larger than the dataset size {len(train_dataset)}\")\n",
    "                    continue\n",
    "\n",
    "                # TrainingArguments 설정\n",
    "                training_args = TrainingArguments(\n",
    "                    output_dir=\"./results\",  # 학습된 모델과 결과를 저장할 경로 설정\n",
    "                    run_name=\"my_experiment\",  # 고유한 run_name 설정  // 로그인 피하기위해\n",
    "                    eval_strategy=\"epoch\",  # 각 에포크마다 평가 수행\n",
    "                    learning_rate=learning_rate,  # 학습률 설정\n",
    "                    per_device_train_batch_size=batch_size,  # 학습 배치 크기 설정\n",
    "                    per_device_eval_batch_size=batch_size,  # 평가 배치 크기 설정\n",
    "                    num_train_epochs=epochs,  # 현재 학습 에포크 수 설정\n",
    "                    weight_decay=0.01,  # 가중치 감쇠 설정\n",
    "                    logging_dir='./logs',  # 로그 저장 경로 설정\n",
    "                    logging_steps=10,  # 로그를 기록할 단계 수 설정\n",
    "                )\n",
    "\n",
    "                # Trainer 생성\n",
    "                trainer = Trainer(\n",
    "                    # model=Classification_model,\n",
    "                    model=Classification_model.to(device),  # 모델을 GPU로 이동\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_dataset,  # 훈련 데이터셋\n",
    "                    eval_dataset=test_dataset,  # 평가 데이터셋\n",
    "                )\n",
    "\n",
    "                # 모델 학습\n",
    "                trainer.train()\n",
    "\n",
    "                # 훈련 데이터에서 평가\n",
    "                train_results = trainer.predict(train_dataset)\n",
    "                train_preds = np.argmax(train_results.predictions, axis=1)\n",
    "                train_labels = train_results.label_ids\n",
    "                train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "                train_precision = precision_score(train_labels, train_preds, average='binary')\n",
    "                train_recall = recall_score(train_labels, train_preds, average='binary')\n",
    "                train_f1 = f1_score(train_labels, train_preds, average='binary')\n",
    "                train_loss = train_results.metrics['eval_loss'] if 'eval_loss' in train_results.metrics else None\n",
    "\n",
    "                # 테스트 데이터에서 평가\n",
    "                eval_results = trainer.evaluate()\n",
    "                predictions = trainer.predict(test_dataset)\n",
    "                preds = np.argmax(predictions.predictions, axis=1)\n",
    "                labels = predictions.label_ids\n",
    "                eval_accuracy = accuracy_score(labels, preds)\n",
    "                eval_precision = precision_score(labels, preds, average='binary')\n",
    "                eval_recall = recall_score(labels, preds, average='binary')\n",
    "                eval_f1 = f1_score(labels, preds, average='binary')\n",
    "                eval_loss = eval_results['eval_loss'] if 'eval_loss' in eval_results else None\n",
    "                tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "                eval_specificity = tn / (tn + fp)\n",
    "\n",
    "                # 성능 메트릭 계산\n",
    "                accuracy = accuracy_score(labels, preds)\n",
    "                precision = precision_score(labels, preds, average='binary')\n",
    "                recall = recall_score(labels, preds, average='binary')\n",
    "                f1 = f1_score(labels, preds, average='binary')\n",
    "                tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "                specificity = tn / (tn + fp)\n",
    "\n",
    "                # 결과 저장\n",
    "                results.append({\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'num_train_epochs': epochs,\n",
    "                    'train_accuracy': train_accuracy,\n",
    "                    'train_precision': train_precision,\n",
    "                    'train_recall': train_recall,\n",
    "                    'train_f1': train_f1,\n",
    "                    'train_loss': train_loss,\n",
    "                    'eval_accuracy': eval_accuracy,\n",
    "                    'eval_precision': eval_precision,\n",
    "                    'eval_recall': eval_recall,\n",
    "                    'eval_f1': eval_f1,\n",
    "                    'eval_loss': eval_loss,\n",
    "                    'eval_specificity': eval_specificity\n",
    "                })\n",
    "\n",
    "                # 최고 성능 모델 기록\n",
    "                if eval_accuracy > best_accuracy:\n",
    "                    best_accuracy = eval_accuracy\n",
    "                    best_F1 = eval_f1\n",
    "                    best_params = {\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'batch_size': batch_size,\n",
    "                        'num_train_epochs': epochs\n",
    "                    }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with parameters: learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "                print(f\"Exception: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "# 최적의 하이퍼파라미터 조합 출력\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "print(\"Best F1:\", best_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfit_or_underfit(results):\n",
    "    for result in results:\n",
    "        train_accuracy = result['train_accuracy']\n",
    "        eval_accuracy = result['eval_accuracy']\n",
    "\n",
    "        # 과적합: 훈련 정확도와 평가 정확도의 차이가 크면 과적합\n",
    "        if train_accuracy - eval_accuracy > 0.1:\n",
    "            status = \"과적합\"\n",
    "\n",
    "        # 과소적합: 훈련 및 평가 정확도가 모두 낮으면 과소적합\n",
    "        elif train_accuracy < 0.7 and eval_accuracy < 0.7:\n",
    "            status = \"과소적합\"\n",
    "\n",
    "        # 정상: 성능 차이가 적당하고 평가 정확도가 높으면 정상 학습\n",
    "        else:\n",
    "            status = \"정상\"\n",
    "\n",
    "        # 결과 출력\n",
    "        print(f\"Learning Rate: {result['learning_rate']}, Batch Size: {result['batch_size']}, Epochs: {result['num_train_epochs']}\")\n",
    "        print(f\"Train Accuracy: {train_accuracy:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")\n",
    "        print(f\"Status: {status}\\n\")\n",
    "\n",
    "\n",
    "# 함수 실행\n",
    "overfit_or_underfit(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "# 혼동행렬 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# 모든 결과 출력\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d932f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# # 최종 모델 평가\n",
    "final_eval_results = trainer.evaluate()\n",
    "\n",
    "# classification_report를 계산하고 딕셔너리로 변환\n",
    "report_dict = classification_report(labels, preds, output_dict=True)\n",
    "\n",
    "# 딕셔너리를 pandas 데이터프레임으로 변환\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "# 데이터프레임을 주피터 노트북에서 보기 좋게 출력\n",
    "styled_report = report_df.style.format(\"{:.2f}\").background_gradient(cmap='Blues')\n",
    "\n",
    "# 최종 평가 결과와 스타일링된 classification_report 출력\n",
    "print(f\"Final Accuracy: {accuracy}\")\n",
    "styled_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 수치 초기화\n",
    "# best_accuracy = 0\n",
    "# best_params = None\n",
    "# results = []\n",
    "\n",
    "# for learning_rate in tqdm(param_grid['learning_rate']):\n",
    "#     for batch_size in param_grid['per_device_train_batch_size']:\n",
    "#         for epochs in param_grid['num_train_epochs']:\n",
    "#             try:\n",
    "#                 if batch_size > len(train_dataset):\n",
    "#                     print(f\"Skipping batch size {batch_size} as it is larger than the dataset size {len(train_dataset)}\")\n",
    "#                     continue\n",
    "\n",
    "#                 # TrainingArguments 설정\n",
    "#                 training_args = TrainingArguments(\n",
    "#                     output_dir=\"./results\",  # 학습된 모델과 결과를 저장할 경로 설정\n",
    "#                     evaluation_strategy=\"epoch\",  # 각 에포크마다 평가 수행\n",
    "#                     learning_rate=learning_rate,  # 학습률 설정\n",
    "#                     per_device_train_batch_size=batch_size,  # 학습 배치 크기 설정\n",
    "#                     per_device_eval_batch_size=batch_size,  # 평가 배치 크기 설정\n",
    "#                     num_train_epochs=epochs,  # 현재 학습 에포크 수 설정\n",
    "#                     weight_decay=0.01,  # 가중치 감쇠 설정\n",
    "#                     logging_dir='./logs',  # 로그 저장 경로 설정\n",
    "#                     logging_steps=10,  # 로그를 기록할 단계 수 설정\n",
    "#                 )\n",
    "\n",
    "#                 # Trainer 생성\n",
    "#                 trainer = Trainer(\n",
    "#                     model=Classification_model,  # 훈련모델 설정\n",
    "#                     args=training_args,\n",
    "#                     train_dataset=train_dataset,  # 훈련 데이터셋\n",
    "#                     eval_dataset=test_dataset,  # 평가 데이터셋\n",
    "#                 )\n",
    "\n",
    "#                 # 모델 학습\n",
    "#                 trainer.train()\n",
    "\n",
    "#                 # 모델 평가\n",
    "#                 eval_results = trainer.evaluate()\n",
    "\n",
    "#                 # 예측 수행\n",
    "#                 predictions = trainer.predict(test_dataset)\n",
    "#                 preds = np.argmax(predictions.predictions, axis=1)  # 모델이 예측한 클래스의 인덱스를 포함하는 배열\n",
    "#                 labels = predictions.label_ids  # 실제 정답 레이블의 인덱스를 포함하는 배열\n",
    "\n",
    "#                 # 평가지표\n",
    "#                 accuracy = accuracy_score(labels, preds)\n",
    "#                 precision = precision_score(labels, preds, average='binary')\n",
    "#                 recall = recall_score(labels, preds, average='binary')\n",
    "#                 f1 = f1_score(labels, preds, average='binary')\n",
    "#                 tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "#                 specificity = tn / (tn + fp)\n",
    "\n",
    "#                 results.append({\n",
    "#                     'learning_rate': learning_rate,\n",
    "#                     'batch_size': batch_size,\n",
    "#                     'num_train_epochs': epochs,\n",
    "#                     'accuracy': accuracy,\n",
    "#                     'precision': precision,\n",
    "#                     'recall': recall,\n",
    "#                     'specificity': specificity,\n",
    "#                     'f1_score': f1\n",
    "#                 })\n",
    "\n",
    "#                 # 정확도를 기준으로 최고모델 저장\n",
    "#                 if accuracy > best_accuracy:\n",
    "#                     best_accuracy = accuracy\n",
    "#                     best_params = {\n",
    "#                         'learning_rate': learning_rate,\n",
    "#                         'batch_size': batch_size,\n",
    "#                         'num_train_epochs': epochs\n",
    "#                     }\n",
    "#                     best_confusion_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error with parameters: learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "#                 print(f\"Exception: {e}\")\n",
    "#                 continue  # 에러 발생 시 다음 파라미터 조합으로 넘어감\n",
    "\n",
    "# # 최적 하이퍼파라미터 조합과 정확도 출력\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282674e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 혼동행렬 시각화\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(best_confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.ylabel('True Labels')\n",
    "# plt.show()\n",
    "\n",
    "# # 모든 결과 출력\n",
    "# for result in results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd140445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 최적 하이퍼파라미터로 다시 학습\n",
    "# best_training_args = TrainingArguments(\n",
    "#     output_dir='./best_model',\n",
    "#     learning_rate=best_params['learning_rate'],\n",
    "#     per_device_train_batch_size=best_params['per_device_train_batch_size'],\n",
    "#     num_train_epochs=best_params['num_train_epochs'],\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "# )\n",
    "\n",
    "# best_trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=best_training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset\n",
    "# )\n",
    "\n",
    "# best_trainer.train()\n",
    "# best_trainer.save_model('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9064d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save_pretrained(\"./kf-deberta-finetuned\")\n",
    "tokenizer.save_pretrained(\"./kf-deberta-finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectM4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
