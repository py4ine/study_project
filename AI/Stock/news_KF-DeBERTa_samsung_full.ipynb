{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1e2ae-07eb-4b55-8062-6a00e421a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from pykospacing import Spacing\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27a7d2-c027-494f-bb67-9a0e848c1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KF-DeBERTa 모델\n",
    "model_name = 'kakaobank/kf-deberta-base'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "Classification_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2진분류\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbbdee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_excel('./data/New_samsung_news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d23091",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[data_df['Outcome'] == '악재', 'labels'] = 0\n",
    "data_df.loc[data_df['Outcome'] == '호재', 'labels'] = 1\n",
    "data_df['labels'] = data_df['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a19402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기, 대소문자 보정 함수\n",
    "spacing = Spacing()\n",
    "def preprocessing(text):\n",
    "    text = spacing(text)\n",
    "    text = text.lower()  # 소문자 변경\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# 기본 불용어 불러오기\n",
    "korean_stopwords_path = \"data/stopwords-ko.txt\"\n",
    "with open(korean_stopwords_path, encoding='utf8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip() for x in stopwords]\n",
    "\n",
    "# 불용어 처리 함수\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()\n",
    "    filtered_text = []\n",
    "    for word in words:\n",
    "        if text not in stopwords:\n",
    "            filtered_text.append(text)\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 및 불용어 처리\n",
    "for i in tqdm(range(len(data_df))):\n",
    "    feature_text = data_df.loc[i, 'summary_content']\n",
    "    processed_text = preprocessing(feature_text)\n",
    "    cleaned_text = remove_stopwords(processed_text, stopwords)\n",
    "    data_df.loc[i, 'processed'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "465f1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.to_excel('./data/data_samsung_processed.xlsx')\n",
    "# data_df = pd.read_excel('./data/data_apple_processed.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6e943c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KF_DeBERT모델 텍스트 토큰화\n",
    "encodings = tokenizer(data_df['processed'].tolist(), truncation=True, padding=True)  # truncation=최대길이를 초과하는 것에 대한 처리\n",
    "data_df['encoding'] = encodings\n",
    "data_df['input_ids'] = encodings['input_ids']  # input_ids=문장을 토크나이저가 일정 단위로 쪼개서 vocab에 들어있는 숫자로 치환한 것\n",
    "data_df['attention_mask'] = encodings['attention_mask']  # attention_mask=attention대상인지 아닌지를 나타냄. 대상이면1, 아니면0, 패딩된건 0으로 학습대상x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.to_excel('./data/data_apple_encoding.xlsx')\n",
    "# data_df = pd.read_excel('./data/data_apple_encoding.xlsx')\n",
    "\n",
    "# # 문자열을 리스트로 변환\n",
    "# data_df['input_ids'] = data_df['input_ids'].str.strip('[]').str.split(', ').apply(lambda x: list(map(int, x)))\n",
    "# data_df['attention_mask'] = data_df['attention_mask'].str.strip('[]').str.split(', ').apply(lambda x: list(map(int, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 비중 조정\n",
    "total_sample = len(data_df)\n",
    "negative = data_df[data_df['labels']==0]  # 부정(악재) 클래스 5,849\n",
    "positive = data_df[data_df['labels']==1]  # 긍정(호재) 클래스 10,672\n",
    "\n",
    "# 데이터 비율 설정\n",
    "total_rate = 1\n",
    "rate = 1\n",
    "negative_sample_size = round(len(negative) * total_rate)\n",
    "positive_sample_size = round(negative_sample_size * total_rate * rate)\n",
    "print(negative_sample_size)\n",
    "print(positive_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07991b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 선택\n",
    "negative_data = random.sample(list(negative.index), negative_sample_size)\n",
    "positive_data = random.sample(list(positive.index), positive_sample_size)\n",
    "\n",
    "# 최종 데이터 인덱스\n",
    "sample_index = negative_data + positive_data\n",
    "random.shuffle(sample_index)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "sample_df = data_df.loc[sample_index]\n",
    "\n",
    "# train_test_split 데이터set\n",
    "train_columns = sample_df[['summary_content', 'input_ids', 'attention_mask']]\n",
    "test_colums = sample_df[['labels']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_columns, test_colums, test_size=0.2)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(X_train.join(y_train))\n",
    "test_dataset = Dataset.from_pandas(X_test.join(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8eecaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 옵션 설정\n",
    "param_grid = {\n",
    "    'learning_rate': [3e-5, 2e-5, 5e-5],  # 학습률\n",
    "    'per_device_train_batch_size': [4, 8, 16, 32],  # 베치크기\n",
    "    'num_train_epochs': [2, 4, 6, 8]  # 에포크 수\n",
    "}\n",
    "\n",
    "# Best Parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'num_train_epochs': 8}\n",
    "# Best Accuracy: 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 초기화\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "for learning_rate in tqdm(param_grid['learning_rate']):\n",
    "    for batch_size in param_grid['per_device_train_batch_size']:\n",
    "        for epochs in param_grid['num_train_epochs']:\n",
    "            try:\n",
    "                if batch_size > len(train_dataset):\n",
    "                    print(f\"Skipping batch size {batch_size} as it is larger than the dataset size {len(train_dataset)}\")\n",
    "                    continue\n",
    "\n",
    "                # TrainingArguments 설정\n",
    "                training_args = TrainingArguments(\n",
    "                    output_dir=\"./results\",  # 학습된 모델과 결과를 저장할 경로 설정\n",
    "                    evaluation_strategy=\"epoch\",  # 각 에포크마다 평가 수행\n",
    "                    learning_rate=learning_rate,  # 학습률 설정\n",
    "                    per_device_train_batch_size=batch_size,  # 학습 배치 크기 설정\n",
    "                    per_device_eval_batch_size=batch_size,  # 평가 배치 크기 설정\n",
    "                    num_train_epochs=epochs,  # 현재 학습 에포크 수 설정\n",
    "                    weight_decay=0.01,  # 가중치 감쇠 설정\n",
    "                    logging_dir='./logs',  # 로그 저장 경로 설정\n",
    "                    logging_steps=10,  # 로그를 기록할 단계 수 설정\n",
    "                )\n",
    "\n",
    "                # Trainer 생성\n",
    "                trainer = Trainer(\n",
    "                    model=Classification_model,  # 훈련모델 설정\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_dataset,  # 훈련 데이터셋\n",
    "                    eval_dataset=test_dataset,  # 평가 데이터셋\n",
    "                )\n",
    "\n",
    "                # 모델 학습\n",
    "                trainer.train()\n",
    "\n",
    "                # 모델 평가\n",
    "                eval_results = trainer.evaluate()\n",
    "\n",
    "                # 예측 수행\n",
    "                predictions = trainer.predict(test_dataset)\n",
    "                preds = np.argmax(predictions.predictions, axis=1)  # 모델이 예측한 클래스의 인덱스를 포함하는 배열\n",
    "                labels = predictions.label_ids  # 실제 정답 레이블의 인덱스를 포함하는 배열\n",
    "\n",
    "                # 평가지표\n",
    "                accuracy = accuracy_score(labels, preds)\n",
    "                precision = precision_score(labels, preds, average='binary')\n",
    "                recall = recall_score(labels, preds, average='binary')\n",
    "                f1 = f1_score(labels, preds, average='binary')\n",
    "                tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "                specificity = tn / (tn + fp)\n",
    "\n",
    "                results.append({\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'num_train_epochs': epochs,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'specificity': specificity,\n",
    "                    'f1_score': f1\n",
    "                })\n",
    "\n",
    "                # 정확도를 기준으로 최고모델 저장\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params = {\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'batch_size': batch_size,\n",
    "                        'num_train_epochs': epochs\n",
    "                    }\n",
    "                    best_confusion_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with parameters: learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "                print(f\"Exception: {e}\")\n",
    "                continue  # 에러 발생 시 다음 파라미터 조합으로 넘어감\n",
    "\n",
    "# 최적 하이퍼파라미터 조합과 정확도 출력\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282674e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동행렬 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(best_confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# 모든 결과 출력\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd140445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 하이퍼파라미터로 다시 학습\n",
    "best_training_args = TrainingArguments(\n",
    "    output_dir='./best_model',\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    per_device_train_batch_size=best_params['per_device_train_batch_size'],\n",
    "    num_train_epochs=best_params['num_train_epochs'],\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "best_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=best_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "best_trainer.train()\n",
    "best_trainer.save_model('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9064d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save_pretrained(\"./kf-deberta-finetuned\")\n",
    "tokenizer.save_pretrained(\"./kf-deberta-finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectM4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
